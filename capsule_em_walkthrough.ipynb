{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# walkthrough capsule em\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import mnist\n",
    "reload(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images, labels = mnist.load_mnist('data/mnist')\n",
    "print('images shape:', images.shape)\n",
    "print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def inspect_mnist(index, images, labels):\n",
    "  plt.title('Index: {:d}, Label: {:d}'.format(\n",
    "    index, labels[index].argmax()\n",
    "  ))\n",
    "  plt.imshow(images[index][:, :, 0], cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inspect_mnist(46, images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.convert_to_tensor(images[:batch_size], dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.convert_to_tensor(labels[:batch_size], dtype=tf.uint8)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input -> conv1\n",
    "conv1 = tf.contrib.layers.conv2d(\n",
    "    x, num_outputs=32, kernel_size=[5, 5], stride=2, padding='VALID'\n",
    ")\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0.,   1.,   2.],\n",
       "         [  3.,   4.,   5.]],\n",
       "\n",
       "        [[ 12.,  13.,  14.],\n",
       "         [ 15.,  16.,  17.]]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aa = tf.constant(\n",
    "#     np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3]\n",
    "# )\n",
    "aa = tf.convert_to_tensor(\n",
    "    np.arange(4 * 4 * 3).reshape((1, 4, 4, 3)), dtype=tf.float32\n",
    ")\n",
    "av = sess.run(aa)\n",
    "av[:, 0:2, 0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   5.,  10.],\n",
       "        [ 15.,  20.,  25.]],\n",
       "\n",
       "       [[ 30.,  35.,  40.],\n",
       "        [ 45.,  50.,  55.]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = tf.convert_to_tensor(\n",
    "    np.arange(2*2*3*5).reshape((2, 2, 3, 5)), dtype=tf.float32\n",
    ")\n",
    "wv = sess.run(ww)\n",
    "wv[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv1:0' shape=(1, 3, 3, 5) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = tf.nn.conv2d(\n",
    "    aa, filter=ww, strides=[1, 1, 1, 1], padding='VALID', name='conv1'\n",
    ")\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 3, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1v = sess.run(conv1)\n",
    "conv1v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4060.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(av[0, 0:2, 0:2, :] * wv[:, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_variable_wrapper(\n",
    "        name, shape=None, dtype=None, initializer=None,\n",
    "        regularizer=None,\n",
    "        trainable=True,\n",
    "        collections=None,\n",
    "        caching_device=None,\n",
    "        partitioner=None,\n",
    "        validate_shape=True,\n",
    "        custom_getter=None\n",
    "):\n",
    "    \"\"\"Wrapper over tf.get_variable().\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(\n",
    "            name, shape=shape, dtype=dtype, initializer=initializer,\n",
    "            regularizer=regularizer, trainable=trainable,\n",
    "            collections=collections, caching_device=caching_device,\n",
    "            partitioner=partitioner, validate_shape=validate_shape,\n",
    "            custom_getter=custom_getter\n",
    "        )\n",
    "    return var\n",
    "\n",
    "\n",
    "def _get_weights_wrapper(name, shape, wd=None):\n",
    "    weights = _get_variable_wrapper(\n",
    "        name=name, shape=shape, dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer(\n",
    "            mean=0.0, stddev=0.05, dtype=tf.float32\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if wd is not None and wd > 0.0:\n",
    "        weights_wd = tf.multiply(\n",
    "            tf.nn.l2_loss(weights), wd, name=name + '/l2loss'\n",
    "        )\n",
    "\n",
    "        tf.add_to_collection('losses', weights_wd)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _get_biases_wrapper(name, shape):\n",
    "    biases = _get_variable_wrapper(\n",
    "        name=name, shape=shape, dtype=tf.float32,\n",
    "        initializer=tf.constant_initializer(0.0)\n",
    "    )\n",
    "\n",
    "    return biases\n",
    "\n",
    "# add an activation?\n",
    "def _conv2d_wrapper(inputs, shape, strides, add_bias, name):\n",
    "    \"\"\"Wrapper over tf.nn.conv2d().\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = _get_weights_wrapper(\n",
    "            name='weights', shape=shape, wd=0.0\n",
    "        )\n",
    "        output = tf.nn.conv2d(\n",
    "            inputs, filter=kernel, strides=strides, padding='SAME', name='conv'\n",
    "        )\n",
    "        if add_bias:\n",
    "            biases = _get_biases_wrapper(\n",
    "                name='biases', shape=[shape[-1]]\n",
    "            )\n",
    "            output = tf.add(\n",
    "                output, biases, name='biasAdd'\n",
    "            )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capsule_init(inputs, shape, strides, capsule_shape, name):\n",
    "    \"\"\"This convert a conv layer to a capsule primary layer.\n",
    "    \n",
    "    Args:\n",
    "        inputs: [N, H, W, C], N: batch_size, ..\n",
    "        shape: kernel shape, say [K, K, I, O]\n",
    "        strides: kernel strides, \n",
    "        capsule_shape: [4, 4]\n",
    "        \n",
    "    Return:\n",
    "        (pose, activation)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(capsule_shape) == 2\n",
    "    \n",
    "    pose = []\n",
    "    for h0 in xrange(capsule_shape[0]):\n",
    "        units = []\n",
    "        for h1 in xrange(capsule_shape[1]):\n",
    "            units.append(_conv2d_wrapper(inputs, shape, strides, False, 'pose_unit_'+str(h0)+'_'+str(h1)))\n",
    "        pose.append(tf.stack(units, axis=-1))\n",
    "    pose = tf.stack(pose, axis=-1)\n",
    "    \n",
    "    activation = _conv2d_wrapper(inputs, shape, strides, False, 'activation'+str(h0)+'_'+str(h1))\n",
    "    \n",
    "    activation = tf.sigmoid(activation)\n",
    "    \n",
    "    return pose, activation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'test/conv:0' shape=(1, 4, 4, 5) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_conv2d_wrapper(aa, shape=[2, 2, 3, 5], strides=[1, 1, 1, 1], add_bias=False, name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pose1, activation1 = capsule_init(aa, shape=[2, 2, 3, 5], strides=[1, 1, 1, 1], capsule_shape=[4, 4], name='primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack_4:0' shape=(1, 4, 4, 5, 4, 4) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sigmoid:0' shape=(1, 4, 4, 5) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pose1v, activation1v = sess.run([pose1, activation1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(3, 4, 4, 5, 4, 4) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose11 = tf.concat([pose1, pose1, pose1], axis=0)\n",
    "pose11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_1:0' shape=(3, 4, 4, 5) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation11 = tf.concat([activation1, activation1, activation1], axis=0)\n",
    "activation11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(1, 4, 4, 5, 4, 4) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = tf.convert_to_tensor(\n",
    "    np.arange(1*4*4*5*4*4).reshape((1, 4, 4, 5, 4, 4)), dtype=tf.float32\n",
    ")\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Tile:0' shape=(3, 4, 4, 5, 4, 4) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w22 = tf.tile(w2, [3, 1, 1, 1, 1, 1])\n",
    "w22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_2:0' shape=(3, 4, 4, 5, 4, 4) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = tf.matmul(pose11, w22)\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = tf.stack([zz, zz], axis=-3)\n",
    "# all i into 1dim\n",
    "z3 = tf.reshape(z2, [3, 80, 2, 4, 4])\n",
    "# all h into 1dim\n",
    "z4 = tf.reshape(z3, [3, 80, 2, 16])\n",
    "\n",
    "z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = tf.constant(1/2, shape=(3, 80, 2), dtype=tf.float32)\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation14 = tf.reshape(activation11, [3, 80])\n",
    "activation14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one iteration\n",
    "tt = 1\n",
    "\n",
    "# m_step\n",
    "# this is one conv op, scan through with stride, to have 14x14 -> 6x6 or what ever\n",
    "# R -> rr, [80, 2], num_i * num_c \n",
    "# a -> activation14, [3, 4, 4, 5] -> [3, 80]\n",
    "# V -> z4, [3, 80, 2, 16]\n",
    "\n",
    "# for cx in 0, 1, \n",
    "# cx==0\n",
    "ri = rr[:, :, 0]\n",
    "rr_prime = ri * activation14\n",
    "\n",
    "mu_h = tf.reduce_sum(z4[:, :, 0, :] * tf.expand_dims(rr_prime, -1), axis=1) / tf.expand_dims(tf.reduce_sum(rr_prime, axis=1), -1)\n",
    "\n",
    "sigma2_h = tf.reduce_sum(\n",
    "    tf.square((z4[:,:,0,:] - tf.expand_dims(mu_h, axis=1))) * tf.expand_dims(rr_prime, -1), \n",
    "    axis=1\n",
    ") / tf.expand_dims(tf.reduce_sum(rr_prime, axis=1), -1)\n",
    "\n",
    "# beta_v should be a trainable_variable\n",
    "beta_v = tf.constant(1, dtype=tf.float32)\n",
    "cost_h = (beta_v + tf.log(sigma2_h)) * tf.expand_dims(tf.reduce_sum(rr_prime, axis=1), -1)\n",
    "\n",
    "# lambda (ll) and beta_a should be trainable variables\n",
    "ll = tf.constant(1, dtype=tf.float32)\n",
    "beta_a = tf.constant(1, dtype=tf.float32)\n",
    "a_c = tf.sigmoid(ll * (beta_a - tf.reduce_sum(cost_h, axis=1)))\n",
    "\n",
    "# this is one value of c -> vectorize to all channels\n",
    "\n",
    "#? for loop to all positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_prime = ri * activation14\n",
    "tf.reduce_sum(rr_prime, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def m_step():\n",
    "    pass\n",
    "\n",
    "def e_step():\n",
    "    pass\n",
    "\n",
    "def capsule_routing_em():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capsule_conv(inputs, shape):\n",
    "    \"\"\"This connect two capsule layers with conv EM routing.\n",
    "    \n",
    "    Args:\n",
    "        inputs: (pose, activation):\n",
    "            - pose: [N, H, W, C, CH, CW], CH, CW are capsule shape CH=4 x CW=4.\n",
    "            - activation: [N, H, W, C]\n",
    "        shape: as conv [H, W, I, O, CH, CW]\n",
    "    \"\"\"\n",
    "    \n",
    "    view = _get_weights_wrapper(\n",
    "        name='weights', shape=shape, wd=0.0\n",
    "    )\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capsule_fc():\n",
    "    \"\"\"EM\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.constant(np.arange(13, 19, dtype=np.int32),\n",
    "                shape=[3, 2])\n",
    "z = tf.matmul(a, b)\n",
    "zv = sess.run(z)\n",
    "\n",
    "y = tf.tensordot(a, b, axes=[[2], [0]])\n",
    "yv = sess.run(y)\n",
    "\n",
    "\n",
    "w1 = tf.constant(\n",
    "    np.arange(3 * 12).reshape(1, 1, 1, 3 * 12), dtype=tf.float32\n",
    ")\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb = tf.matmul(aa, w1)\n",
    "\n",
    "# 1 x 1 x 32 x 4 x 4\n",
    "\n",
    "# how many W? 32\n",
    "\n",
    "# pose\n",
    "# [batch_size, 4, 4, 6, 6, 11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caspule_primary(input, kernel=[2, 2], h=[4, 4], activation_fn=tf.sigmoid):\n",
    "    pass # for loop then tf.stack\n",
    "    # return pose [batch_size, h0, h1, h, w, d], activation [batch_size, h, w, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capsule_convolution():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bv = sess.run(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for TF Graph visualization\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "  \n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_graph(graph_def=graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
