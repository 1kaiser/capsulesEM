## Matrix Capsules with EM Routing - Implementation

### The Framework

Ok, let me first define some notations for consistency. These should be the common notation.

1. Tensor with shape [N, H, W, C], often N is the number of samples (batch_size), H is the image height,
W is the image width, and C is the number of channels. For example, in MNIST, each image is 28x28, with a single channel,
so a batch size of 128 images would lead to an input tensor of [128, 28, 28, 1].

1. Kernel shape with [KH, KW, I, O], often KH is the kernel height, KW is the kernel width, I is the number of input channels
and O is the number of output channels, e.g., a 5x5 convolution on the previous [128, 28, 28, 1] to provide 32 channels ouptut
would require a kernel shape [5, 5, 1, 32]. Of course, we also need strides to determine the output H and W.

1. Tensor with shape [N, H, W, C, PH, PW]. This is a matrix capsules layers, extending from the regular layers, where PH and PW
are the pose height and pose activation. In this paper, PH = PW = 4.

1. Kernel shape with [KH, KW, I, O, PH, PW]. This is a matrix capsules convolution kernel, extending from the regular kernel,
where PH and PW are the pose height and pose activation. In this paper, PH = PW = 4.

Now, let's see how to build a neural network with matrix capsules.

![Figure 1](fig/capsulesEM-Figure1.png)

To make it clear, let's names each layer as layerIL (input layer), layerAL, layerBL, layerCL, layerDL, and layerEL, 
from left to right. In the paper, the author uses A, B, C, D, E to present the number of channels at each layer, so 
I add an `L` at the end of layer names to distinguish the name of layer from the number of channels.

In the following, I will let A=32, B=48, C=64, D=80, and E=10, so that we can distinguish between each layers.
In the paper, A=B=C=D=32 and E=5. 

#### Overview

In a summary, the shape of each layer and kernel:

LayerIL: Let's assume we use MNIST, and have a batch_size 128, [128, 28, 28, 1]

LayerIL -> LayerAL (A=32): kernel [5, 5, 1, 32], strides [1, 2, 2, 1], ReLU, padding VALID.

LayerAL: [128, 12, 12, 32]

LayerAL -> LayerBL (B=48): kernel [1, 1, 32, 48] x (4 x 4 + 1), we need 17 such kernels of [1, 1, 32, 32], 
16 for building poses and 1 for building activation. In this step, strides are [1, 1, 1, 1].

LayerBL: pose [128, 12, 12, 48, 4, 4], activation [128, 12, 12, 48]

LayerBL -> LayerCL (C=64): kernel [3, 3, 48, 64, 4, 4], e.g., K = 3. strides, [1, 2, 2, 1].

LayerCL: pose [128, 5, 5, 64, 4, 4], activation [128, 5, 5, 64]

LayerCL -> LayerDL (D=80): kernel [3, 3, 64, 80, 4, 4], strides [1, 1, 1, 1].

LayerDL: pose [128, 5, 5, 80, 4, 4], activation [128, 5, 5, 80]

LayerDL -> LayerEL (E=10), `kernel` [1, 1, 80, 10, 4, 4], FC-EM. This is different from the layerBL->layerCL and layerCL->layerDL.
In layerBL->layerCL and layerCL->layerDL, the EM is within the 3 (kernel KH) x 3 (kernel KW) x 48 or 3 x 3 x 64. In layerDL->layerEL, the EM is within 
5 (entire H) x 5 (entire W) x 80.

LayerEL -> pose [128, 10, 4, 4], activation [128, 10].

#### LayerIL -> LayerAL

LayerIL: Let's assume we use MNIST, and have a batch_size 128, [128, 28, 28, 1]

LayerIL -> LayerAL (A=32): kernel [5, 5, 1, 32], strides [1, 2, 2, 1], ReLU, padding VALID.

This is a regular convoluation operation between layerIL and layerAL.

#### LayerAL -> LayerBL

LayerAL: [128, 12, 12, 32]

LayerAL -> LayerBL (B=48): kernel [1, 1, 32, 48] x (4 x 4 + 1), we need 17 such kernels of [1, 1, 32, 32], 
16 for building poses and 1 for building activation. In this step, strides are [1, 1, 1, 1].

This involves how to build a primary capsules from a regular convolution layer.

Let's imagine we do the regular convolution with kernel [1, 1, 32, 48]. Do it once, we obtain
a layerBL0, with [128, 12, 12, 48]. 

We repeat this 16 times, then we have layerBL0, ..., layerBL15, each has the shape [128, 12, 12, 48]. 

We can stack layerBL0, layerBL1, layerBL2, layerBL3 into a tensor of [128, 12, 12, 48, 4], call it layerBL-S0.
We can stack layerBL4, layerBL5, layerBL6, layerBL7 into a tensor of [128, 12, 12, 48, 4], call it layerBL-S1.
We can stack layerBL8, layerBL9, layerBL10, layerBL11 into a tensor of [128, 12, 12, 48, 4], call it layerBL-S2.
We can stack layerBL12, layerBL13, layerBL14, layerBL15 into a tensor of [128, 12, 12, 48, 4], call it layerBL-S3.

We can then stack layerBL-S0, layerBL-S1, layerBL-S2, and layerBL-S3 into [128, 12, 12, 48, 4, 4].

Of course, in practice, it is equivalent to use a kernel [1, 1, 32, 48 x 16] to get an output [128, 12, 12, 48 x 16],
and reshape it to [128, 12, 12, 48, 4, 4].

The activation is obtained use kernel [1, 1, 32, 48], strides [1, 1, 1, 1], no bias added, and tf.sigmoid as activation 
function. 

LayerBL: pose [128, 12, 12, 48, 4, 4], activation [128, 12, 12, 48]

LayerBL -> LayerCL (C=64): kernel [3, 3, 48, 64, 4, 4], e.g., K = 3. strides, [1, 2, 2, 1].

#### Conv: LayerBL -> LayerCL (C=64)

LayerBL: pose [128, 12, 12, 48, 4, 4], activation [128, 12, 12, 48]

LayerBL -> LayerCL (C=64): kernel [3, 3, 48, 64, 4, 4], e.g., K = 3. strides, [1, 2, 2, 1].

This is the key part, the convolution operation between matrix capsules.

The inputs are pose [128, 12, 12, 48, 4, 4], and activation [128, 12, 12, 48]

We first initialize the weight matrix W [128, 3, 3, 48, 4, 4]

Take a sub-region (the K x K blue region in layerBL), do the pose-vote transformation, and use vote, activation for EM routing.

In specific,

```
for hh in (0, 2, 4, ... 8):
    for ww in (0, 2, 4, ..., 8):
        inputs_pose_slice = inputs_pose[:, hh:(hh+3), ww:(ww+3), :, :, :]
        # inputs_pose_slice is [N, KH, KW, I, PH, PW]
        # expand dim of input_pose_slice to [N, KH, KW, I, 1, PH, PW]
        inputs_pose_slice_expansion = tf.expand_dims(inputs_pose_slice, axis=4)
        # inputs_vote [N, KH, KW, I, O, PH, PW]
        inputs_vote = inputs_pose_slice_expansion x W
        # use inputs_vote, inputs_activation to obtain output pose, activation
        # pose is [N, 1, 1, O, PH, PW], activation is [N, 1, 1, O]
        # this are the pose and activation at one position across all channels
        pose, activation = matrix_em_routing(inputs_vote, inputs_activation)
# the double for loop is building layerCL one at a time.
        
```

#### FC: LayerDL -> LayerEL (E=10)

This is confusing from the figure that the Kernel seems to have [1, 1, 80, 10],
so the output layer should be [128, 5, 5, 10, 4, 4], instead of [128, 10, 4, 4].

The reason is this is really a fully-connected layer, __with shared kernel weight of [1, 1, 80, 10]__.

In specific, we initialize a kernel weight of [1, 1, 80, 10], at each 1 x 1 patch of input, say, [128, 0:1, 0:1, 80, 4, 4], 
we expand dimension of that input slice, and multiply with the weight to have the vote. 
Thus, we have the vote [128, 5, 5, 80, 10, 4, 4]. We reshape the vote into [128, 5 x 5 x 80, 10, 4, 4],
and we reshape the inputs activation into [128, 5 x 5 x 80].

Then, we use the EM routing to obtain the output pose [128, 10, 4, 4] and activation [128, 10].

In summary, the difference is

1. From the layerBL->layerCL and layerCL->layerDL, the kernel is [K, K, I, O, PH, PW], 
the weights are shared between difference slice. 

    In layerDL->layerEL, the kernel is [K = 1, K = 1, I, O, PH, PW] to ensure that __share the transformation
    matrices between different positions of the same capsule type.__

2. From the layerBL->layerCL and layerCL->layerDL, the EM is within the 3 (kernel KH) x 3 (kernel KW) x 48 or 3 x 3 x 64. 

    In layerDL->layerEL, the EM is within 5 (entire H) x 5 (entire W) x 80.
 
Note: remember to add scaled coordinate to the first two elements of its vote.

### The EM Routing

As discussed, the inputs are votes [N, KH, KW, I, O, PH, PW] and activations [N, KH, KW, I].

In paper, the author describes the EM routing algorithm in a way that in 
M-step, updating one output mean ($\mu$), ($\simga$) and activation (a) using all inputs to this outputs
routing value (R_{:c:}) and vote (V_{:c:}).
In E-step, updating the routing of one input to all outputs.

Here, in  I change the order of inputs, vote first, then activation. vote is the V tensor, activation is the a vector in paper.
I also assume inputs vote, acitvations are tensors with votes reshaped into [N, KH x KW x I, O, PH x PW], 
inputs activations reshaped into [N, KH x KW x I, 1, 1]. I will describe the algorithm in a vectorized way.


```
matrix_em_routing(vote, i_activation):

```

